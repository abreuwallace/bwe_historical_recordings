defaults:
  - dset: chopin

path_experiment: "/home/wallace.abreu/Mestrado/behm-gan_vanilla/experiments_bwe/chopin/"
path_experiment_denoiser: /home/wallace.abreu/Mestrado/behm-gan_vanilla/experiments_denoiser/"
tensorboard_logs: "/home/wallace.abreu/Mestrado/behm-gan_vanilla/tb_logs/chopin"
# Dataset related
fs: 44100  #default is 44100, better NEVER change
seg_len_s_train: 2 #length of the train (and val) segments in seconds
seg_len_s_val: 2 #length of the train (and val) segments in seconds
seg_len_s_test: 2 #length of the train (and val) segments in seconds
freq_inference: 10 #we do inference after * epochs
num_val_audios: 4 #number of audios to inference in validation and save in tensorboard
freq_save: 2 #we save the model after x iteration

num_real_test_segments: 5 #number of real recordings inferenced every epoch
num_test_segments: 5
buffer_size: 1000 # buffer size for shuffling datasets (train and val)
# Dataset Augmentation
overlap: 0   #overlap when extracting audio segments, default is 0, augment if more data is needed

# Logging and printing, and does not impact training
#device: cuda
verbose: 0
use_tensorboard: True


num_workers: 4


# Checkpointing, by default automatically load last checkpoint
checkpoint: "/home/wallace.abreu/Mestrado/behm-gan_vanilla/experiments_bwe/chopin/checkpoint_gen"
checkpoint_denoiser: "checkpoint_29"
continue_from: '' # Path the a checkpoint.th file to start from.
                  # this is not used in the name of the experiment!
                  # so use a dummy=something not to mixup experiments.
continue_best: false  # continue from best, not last state if continue_from is set.
only_inference: false

# Optimization related
optim: adam
lr: 2e-4 #used
beta1: 0.5 #used
beta2: 0.9 #used

#just for the denoiser
variable_lr: True 

epochs: 800
batch_size: 2
multi_batch: 1
batch_size_val: 1
batch_size_test: 1
minibatches: 1
val_take: -1
steps_per_epoch: 2000

#denoiser:
#  checkpoint: "please specify this path parameter" 

#STFT parameteres
stft:
  win_size: 2048     #STFT window size
  hop_size:  512

#inference param
inference:
  exp_name: "noname"
  use_denoiser: False
  use_bwe: True
  apply_lpf: True
  use_butter_filter: False
  no_filter: False
  fc: 3000
  audio: None
# Models
model: unet # either demucs or dwave

unet1d:
  depth: 8

unet_generator:
  activation: "elu"
  use_weight_norm: True
  use_SAM: True
  use_fencoding: True
  num_tfc: 3
  num_stages: 1
  depth: 6
  f_dim: 1025 #reduced vesion #hardcoded, depends on the stft window
  Ns: [64,128,128,128,256,512,512,512]
  ksize_init: [7,7]
  ksize_encoder: [3,3]
  ksize_hc: [7,7]
  ksize_decoder: [3,3]
  ksize_bn: [3,3]

denoiser:
  activation: "elu"
  use_csff: False
  use_SAM: True
  use_cam: False
  use_fam: False
  use_fencoding: True
  use_tdf: False
  use_alttdfs: False
  num_tfc: 3
  num_stages: 2
  depth: 6
  f_dim: 1025 #hardcoded, depends on the stft window

bwe:
  use_msd_disc: True
  use_NLDs: False
  lpf:
    mean_fc: 3000
    std_fc: 0
    num_random_filters: 1
    filters_csv: "/home/wallace.abreu/Mestrado/behm-gan_vanilla/experiments_bwe"
    remez_order: 1000
    remez_trans: 200 #transition width in Hz
  add_noise:
    add_noise: True
    power: .001
  lambda_adv: 1
  lambda_freq_loss: 0.4
  lambda_l1_loss: 0 #l1 loss only for low-passed
  lambda_l2_loss: 0 #l1 loss only for low-passed
  adv_loss: "LS" #"LS", "hinge" try also "WGANGP"
  use_mse_loss: False
  repeat_disc_training: 2
  epochs_in_stage_l1: 50
  lr_gen_l1: 1e-4
  lr_gen_l2: 1e-5
  lr_dis_l2: 1e-4
  load_model: False
  discriminators:
    adamD:
      beta1: 0.5
      beta2: 0.9
  generator:
    variant: "unet2d"   #"audiounet", "seanet", "unet2d"

# Hydra config
hydra:
  job:
    config:
      # configuration for the ${hydra.job.override_dirname} runtime variable
      override_dirname:
        kv_sep: '='
        item_sep: ','
        # Remove all paths, as the / in them would mess up things
        exclude_keys: ['path_experiment',
          'hydra.job_logging.handles.file.filename']
